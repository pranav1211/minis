[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.17.1","content-config-digest","9935d8234f21950d","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://minis.beyondmebtw.com\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false}}","posts",["Map",11,12,34,35,57,58,78,79,101,102,134,135,153,154,174,175],"mini_4o458m",{"id":11,"data":13,"body":22,"filePath":23,"digest":24,"rendered":25},{"id":11,"title":14,"date":15,"time":16,"tags":17},"A Realization : Lumex Project","2025-09-03","22:54",[18,19,20,21],"project","computer vision","ai","software","This is a continuation of me working on my big project, and Iâ€™ve made some progress. I broke things down to the core basics of what Iâ€™m actually trying to do. After some deep digging (and a stressful back-and-forth with AI), I realized that the repo I was relying on which I already knew was 4 years old, was simply too flawed to sit and fix. I tried starting from scratch, but then I hit the real issue: **AI itself.**\n\nAI is good, no doubt. Without it, I wouldnâ€™t have even thought of this project, let alone gotten this far without a complete breakdown. But the problem is, it just kept giving me the same outdated suggestions again and again.\n\nIf thereâ€™s one thing this project has taught me so far, itâ€™s how much software can make your life miserable. Documentation is scarce, libraries change, support is thin, and when youâ€™re trying to build something that hasnâ€™t really been done before, itâ€™s brutally difficult. The panic-filled sleeps and the dreams of failing have been haunting me since Sunday (31/8/25). Talking things over with my father, my accomplishments and failures alike has helped. Even if the problem doesnâ€™t get solved, just talking about it helps. Thereâ€™s really no one else I can share this with, so Iâ€™m grateful I at least have that.\n\nNow, onto the progress. After going in circles, I finally snapped and told ChatGPT straight: *â€œIâ€™m running in loops here. Iâ€™m doing the same thing again and again. All your references are outdated â€” 4 years old. Existing libraries donâ€™t work due to changes. Stop giving me the same stuff and letâ€™s actually think through how this can work.â€*\n\nIt paused for over four minutes and finally gave me the right direction. The solution: there **does** exist a Java API for DepthAI. It is : (JAVA CPP DEPTHAI Preset)[https://github.com/bytedeco/javacpp-presets/tree/master/depthai] It was last updated just 10 months ago, and seeing it still being used in 2024 is a good sign. From there, I broke things down even further. Now my current version is basically just checking if the device is connected no OpenCV, nothing extra, just DepthAI.\n\nAfter some back and forth with settings (and still battling the AI feeding me old information), I managed to create an APK that actually compiled the DepthAI library and built successfully. The only issue is that it was built under the assumption that the PC, phone, and OAK were all connected together. The APK ended up being around 400 MB, which isnâ€™t a big deal, but now the next step is to strip it down: build a minimal UI and get this raw functionality working.\n\nFor now, Iâ€™ve had to pause because of some external device issues, but Iâ€™ll get back to it soon. In the meantime, I want to focus, reflect, and most importantly get my latest F1 recap out.\n\nAnd thatâ€™s where my progress stands. If nothing else, Iâ€™ve already learned that frustration and failure are just part of the process. The important thing is that Iâ€™m still moving, even when it feels like Iâ€™m standing still.","src/content/posts/mini_4o458m.md","e8da84dfa7909c9b",{"html":26,"metadata":27},"\u003Cp>This is a continuation of me working on my big project, and Iâ€™ve made some progress. I broke things down to the core basics of what Iâ€™m actually trying to do. After some deep digging (and a stressful back-and-forth with AI), I realized that the repo I was relying on which I already knew was 4 years old, was simply too flawed to sit and fix. I tried starting from scratch, but then I hit the real issue: \u003Cstrong>AI itself.\u003C/strong>\u003C/p>\n\u003Cp>AI is good, no doubt. Without it, I wouldnâ€™t have even thought of this project, let alone gotten this far without a complete breakdown. But the problem is, it just kept giving me the same outdated suggestions again and again.\u003C/p>\n\u003Cp>If thereâ€™s one thing this project has taught me so far, itâ€™s how much software can make your life miserable. Documentation is scarce, libraries change, support is thin, and when youâ€™re trying to build something that hasnâ€™t really been done before, itâ€™s brutally difficult. The panic-filled sleeps and the dreams of failing have been haunting me since Sunday (31/8/25). Talking things over with my father, my accomplishments and failures alike has helped. Even if the problem doesnâ€™t get solved, just talking about it helps. Thereâ€™s really no one else I can share this with, so Iâ€™m grateful I at least have that.\u003C/p>\n\u003Cp>Now, onto the progress. After going in circles, I finally snapped and told ChatGPT straight: \u003Cem>â€œIâ€™m running in loops here. Iâ€™m doing the same thing again and again. All your references are outdated â€” 4 years old. Existing libraries donâ€™t work due to changes. Stop giving me the same stuff and letâ€™s actually think through how this can work.â€\u003C/em>\u003C/p>\n\u003Cp>It paused for over four minutes and finally gave me the right direction. The solution: there \u003Cstrong>does\u003C/strong> exist a Java API for DepthAI. It is : (JAVA CPP DEPTHAI Preset)[\u003Ca href=\"https://github.com/bytedeco/javacpp-presets/tree/master/depthai\">https://github.com/bytedeco/javacpp-presets/tree/master/depthai\u003C/a>] It was last updated just 10 months ago, and seeing it still being used in 2024 is a good sign. From there, I broke things down even further. Now my current version is basically just checking if the device is connected no OpenCV, nothing extra, just DepthAI.\u003C/p>\n\u003Cp>After some back and forth with settings (and still battling the AI feeding me old information), I managed to create an APK that actually compiled the DepthAI library and built successfully. The only issue is that it was built under the assumption that the PC, phone, and OAK were all connected together. The APK ended up being around 400 MB, which isnâ€™t a big deal, but now the next step is to strip it down: build a minimal UI and get this raw functionality working.\u003C/p>\n\u003Cp>For now, Iâ€™ve had to pause because of some external device issues, but Iâ€™ll get back to it soon. In the meantime, I want to focus, reflect, and most importantly get my latest F1 recap out.\u003C/p>\n\u003Cp>And thatâ€™s where my progress stands. If nothing else, Iâ€™ve already learned that frustration and failure are just part of the process. The important thing is that Iâ€™m still moving, even when it feels like Iâ€™m standing still.\u003C/p>",{"headings":28,"localImagePaths":29,"remoteImagePaths":30,"frontmatter":31,"imagePaths":33},[],[],[],{"id":11,"title":14,"date":15,"time":16,"tags":32},[18,19,20,21],[],"mini_4oqc50",{"id":34,"data":36,"body":41,"filePath":42,"digest":43,"rendered":44},{"id":34,"title":37,"date":38,"time":39,"tags":40},"Decent Progress : Lumex Project","2025-09-11","21:18",[18,19,20,21],"**Progress Update**  \nOver the past couple of days, I focused on getting the DepthAI AAR properly configured and integrated. For context, the AAR is essentially a packaged library (like a zip file) that contains the critical components of the DepthAI module, including the firmware and essential functionality.\n\nPreviously, I discovered that the Bytedeco C++ library worked with DepthAI but only supported the camera itself. not the USB connection. The publicly available AAR only supports USB. This led me to create a **custom AAR** that works for both the camera and USB.\n\nThe AAR build is now **successful**. Initially, there were compilation issues where only a single class file was being compiled while the others were skipped. After cleaning the Gradle cache, double-checking syntax, and following some debugging references, all class files are now correctly included in the AAR.\n\nWith the new AAR integrated, I managed to reach a working APK where both the USB connection and DepthAI function properly. Logs now show **â€œOAK connectedâ€** and **â€œPipeline startedâ€**, confirming that real communication is happening between the phone and the device and no dummy test is happening.\n\n---\n\n**Current Challenge: RGB Streaming**  \nThe next milestone is streaming RGB video to the phone. However, I ran into issues. likely due to a firmware mismatch. While communication works, the streaming pipeline isnâ€™t functioning correctly. I attempted fixes but ended up in loops, so Iâ€™ve paused this part for now.\n\nMoving forward, I am rolling back to the **pipeline-check checkpoint** and rebuild from there, ensuring stability before moving onto streaming. Iâ€™m also moving the project to **IntelliJ**, as itâ€™s suggested to be more efficient for this workflow compared to Android Studio. Some configuration issues came up, but Iâ€™m confident theyâ€™ll be resolved soon.\n\n---\n\n### **Reflections and Learnings**\n\nItâ€™s been a week since I started this intensive phase of the project. effectively my second week. Progress isnâ€™t measured by speed alone, the issues Iâ€™m encountering would take teams to resolve. Fortunately, Iâ€™m not alone in this my father has been assisting me, offering 35 years of software development experience. Talking to him gives me perspective and motivation when things feel overwhelming.\n\nMentally, Iâ€™m stable, but there are moments of fear and self-doubt that made it hard to open Android Studio some days. Even when I make progress, my heart races, and I feel momentarily lost. Yet, Iâ€™m still moving forward, which feels like a core part of me saying, *â€œThe only way forward is through.â€*\n\nThis project is changing me in ways I donâ€™t fully understand yet. I know that taking breaks is important, but equally important is coming back from them with determination, rather than letting fear take hold. Despite challenges, I feel a quiet confidence that this journey is worth every bit of effort.","src/content/posts/mini_4oqc50.md","efb3f6186034e34f",{"html":45,"metadata":46},"\u003Cp>\u003Cstrong>Progress Update\u003C/strong>\u003Cbr>\nOver the past couple of days, I focused on getting the DepthAI AAR properly configured and integrated. For context, the AAR is essentially a packaged library (like a zip file) that contains the critical components of the DepthAI module, including the firmware and essential functionality.\u003C/p>\n\u003Cp>Previously, I discovered that the Bytedeco C++ library worked with DepthAI but only supported the camera itself. not the USB connection. The publicly available AAR only supports USB. This led me to create a \u003Cstrong>custom AAR\u003C/strong> that works for both the camera and USB.\u003C/p>\n\u003Cp>The AAR build is now \u003Cstrong>successful\u003C/strong>. Initially, there were compilation issues where only a single class file was being compiled while the others were skipped. After cleaning the Gradle cache, double-checking syntax, and following some debugging references, all class files are now correctly included in the AAR.\u003C/p>\n\u003Cp>With the new AAR integrated, I managed to reach a working APK where both the USB connection and DepthAI function properly. Logs now show \u003Cstrong>â€œOAK connectedâ€\u003C/strong> and \u003Cstrong>â€œPipeline startedâ€\u003C/strong>, confirming that real communication is happening between the phone and the device and no dummy test is happening.\u003C/p>\n\u003Chr>\n\u003Cp>\u003Cstrong>Current Challenge: RGB Streaming\u003C/strong>\u003Cbr>\nThe next milestone is streaming RGB video to the phone. However, I ran into issues. likely due to a firmware mismatch. While communication works, the streaming pipeline isnâ€™t functioning correctly. I attempted fixes but ended up in loops, so Iâ€™ve paused this part for now.\u003C/p>\n\u003Cp>Moving forward, I am rolling back to the \u003Cstrong>pipeline-check checkpoint\u003C/strong> and rebuild from there, ensuring stability before moving onto streaming. Iâ€™m also moving the project to \u003Cstrong>IntelliJ\u003C/strong>, as itâ€™s suggested to be more efficient for this workflow compared to Android Studio. Some configuration issues came up, but Iâ€™m confident theyâ€™ll be resolved soon.\u003C/p>\n\u003Chr>\n\u003Ch3 id=\"reflections-and-learnings\">\u003Cstrong>Reflections and Learnings\u003C/strong>\u003C/h3>\n\u003Cp>Itâ€™s been a week since I started this intensive phase of the project. effectively my second week. Progress isnâ€™t measured by speed alone, the issues Iâ€™m encountering would take teams to resolve. Fortunately, Iâ€™m not alone in this my father has been assisting me, offering 35 years of software development experience. Talking to him gives me perspective and motivation when things feel overwhelming.\u003C/p>\n\u003Cp>Mentally, Iâ€™m stable, but there are moments of fear and self-doubt that made it hard to open Android Studio some days. Even when I make progress, my heart races, and I feel momentarily lost. Yet, Iâ€™m still moving forward, which feels like a core part of me saying, \u003Cem>â€œThe only way forward is through.â€\u003C/em>\u003C/p>\n\u003Cp>This project is changing me in ways I donâ€™t fully understand yet. I know that taking breaks is important, but equally important is coming back from them with determination, rather than letting fear take hold. Despite challenges, I feel a quiet confidence that this journey is worth every bit of effort.\u003C/p>",{"headings":47,"localImagePaths":52,"remoteImagePaths":53,"frontmatter":54,"imagePaths":56},[48],{"depth":49,"slug":50,"text":51},3,"reflections-and-learnings","Reflections and Learnings",[],[],{"id":34,"title":37,"date":38,"time":39,"tags":55},[18,19,20,21],[],"mini_cxq0pw",{"id":57,"data":59,"body":66,"filePath":67,"digest":68,"rendered":69},{"id":57,"title":60,"date":61,"time":62,"tags":63},"The Problem is Me (and So Is the Solution)","2025-11-30","23:52",[64,18,65],"blog","growth","Iâ€™m finally back with a live blog, itâ€™s been a while. Iâ€™ve still been writing: show reviews, bits about life, and recently, that whole journey with the DepthAI project. But this one hits differently. This one comes after getting hit with a brick. Not literallyâ€¦ though honestly, that might feel easier than what this past quarter has been.\n\nWhen I look back at the last three months, Iâ€™ve been carrying the entire software worldâ€™s sins on my shoulders. If Android has an error, **I** feel broken. My dad told me straight: I need to stop taking every bug personally. And heâ€™s right, Iâ€™ve tied every failure to who I am. Itâ€™s gotten bad enough that I donâ€™t work because I fear either succeedingâ€¦ or failing again.\n\nWhat is worse? I chase errors now. Because if I find one, I get a free pass to delay everything till the next big disaster. And Iâ€™ve been doing exactly that.\n\nIâ€™m scared. **Really** scared. And itâ€™s not like Iâ€™m behind, everything is up to date, files ready, environment set. One click to build and I still backed away.\n\nIf I keep running from problems like this, why am I even in software? This is a crisis of confidence, plain and simple, It's the worry that people will think less of meâ€¦ or worse, that **I** will.\n\nThere will always be problems. Thatâ€™s the job. But once you solve them, theyâ€™re gone. I donâ€™t have to carry them forever.\n\nSo Iâ€™m leaving all that weight behind. Not looking back. Just driving forward â€” fast, focused, relentless until I get tired. And even when I do, Iâ€™ll rest without regret. As Elsa said \"Let It Go!\"\n\nItâ€™s a new month, the final stretch of the year. Iâ€™m not waiting for signs or motivation or fate to approve my progress. Iâ€™m done with that.\n\nIâ€™m capable. Iâ€™ve got this.\n\nAnd yes â€” expect the DepthAI project to be up and running very soon.","src/content/posts/mini_cxq0pw.md","1560f15060e65430",{"html":70,"metadata":71},"\u003Cp>Iâ€™m finally back with a live blog, itâ€™s been a while. Iâ€™ve still been writing: show reviews, bits about life, and recently, that whole journey with the DepthAI project. But this one hits differently. This one comes after getting hit with a brick. Not literallyâ€¦ though honestly, that might feel easier than what this past quarter has been.\u003C/p>\n\u003Cp>When I look back at the last three months, Iâ€™ve been carrying the entire software worldâ€™s sins on my shoulders. If Android has an error, \u003Cstrong>I\u003C/strong> feel broken. My dad told me straight: I need to stop taking every bug personally. And heâ€™s right, Iâ€™ve tied every failure to who I am. Itâ€™s gotten bad enough that I donâ€™t work because I fear either succeedingâ€¦ or failing again.\u003C/p>\n\u003Cp>What is worse? I chase errors now. Because if I find one, I get a free pass to delay everything till the next big disaster. And Iâ€™ve been doing exactly that.\u003C/p>\n\u003Cp>Iâ€™m scared. \u003Cstrong>Really\u003C/strong> scared. And itâ€™s not like Iâ€™m behind, everything is up to date, files ready, environment set. One click to build and I still backed away.\u003C/p>\n\u003Cp>If I keep running from problems like this, why am I even in software? This is a crisis of confidence, plain and simple, Itâ€™s the worry that people will think less of meâ€¦ or worse, that \u003Cstrong>I\u003C/strong> will.\u003C/p>\n\u003Cp>There will always be problems. Thatâ€™s the job. But once you solve them, theyâ€™re gone. I donâ€™t have to carry them forever.\u003C/p>\n\u003Cp>So Iâ€™m leaving all that weight behind. Not looking back. Just driving forward â€” fast, focused, relentless until I get tired. And even when I do, Iâ€™ll rest without regret. As Elsa said â€œLet It Go!â€\u003C/p>\n\u003Cp>Itâ€™s a new month, the final stretch of the year. Iâ€™m not waiting for signs or motivation or fate to approve my progress. Iâ€™m done with that.\u003C/p>\n\u003Cp>Iâ€™m capable. Iâ€™ve got this.\u003C/p>\n\u003Cp>And yes â€” expect the DepthAI project to be up and running very soon.\u003C/p>",{"headings":72,"localImagePaths":73,"remoteImagePaths":74,"frontmatter":75,"imagePaths":77},[],[],[],{"id":57,"title":60,"date":61,"time":62,"tags":76},[64,18,65],[],"mini_ffuskn",{"id":78,"data":80,"body":86,"filePath":87,"digest":88,"rendered":89},{"id":78,"title":81,"date":82,"time":83,"tags":84},"The First of Many","2025-08-20","20:02",[64,85,18],"first","# The First of Many\n\n---\n\nThe wait is finally over!  \n\nAfter 2 weeks, and roughly 12 hours of development â€” itâ€™s here. I present to you **Minis by *Beyond Me Btw* **.  \n\nA live blogging platform where I can share quick updates and thoughts that donâ€™t need a fully produced blog post. It could be a small discovery in tech, a movie review I didnâ€™t enjoy, or just something I feel like talking about.  \n\nWith Minis, I can give live commentary on projects without waiting until the very end.  \n\nAnd this is just the start. Iâ€™ve got more features planned, maybe even a Twitter bot for fun, and plenty more to come.  \n\n**Stay Tuned, The best is yet to come!**\n\n![Minis by Beyond Me Btw](https://content.beyondmebtw.com/experience/exp30.webp)","src/content/posts/mini_ffuskn.md","5bf83c8fc30b63ae",{"html":90,"metadata":91},"\u003Ch1 id=\"the-first-of-many\">The First of Many\u003C/h1>\n\u003Chr>\n\u003Cp>The wait is finally over!\u003C/p>\n\u003Cp>After 2 weeks, and roughly 12 hours of development â€” itâ€™s here. I present to you **Minis by \u003Cem>Beyond Me Btw\u003C/em> **.\u003C/p>\n\u003Cp>A live blogging platform where I can share quick updates and thoughts that donâ€™t need a fully produced blog post. It could be a small discovery in tech, a movie review I didnâ€™t enjoy, or just something I feel like talking about.\u003C/p>\n\u003Cp>With Minis, I can give live commentary on projects without waiting until the very end.\u003C/p>\n\u003Cp>And this is just the start. Iâ€™ve got more features planned, maybe even a Twitter bot for fun, and plenty more to come.\u003C/p>\n\u003Cp>\u003Cstrong>Stay Tuned, The best is yet to come!\u003C/strong>\u003C/p>\n\u003Cp>\u003Cimg src=\"https://content.beyondmebtw.com/experience/exp30.webp\" alt=\"Minis by Beyond Me Btw\">\u003C/p>",{"headings":92,"localImagePaths":96,"remoteImagePaths":97,"frontmatter":98,"imagePaths":100},[93],{"depth":94,"slug":95,"text":81},1,"the-first-of-many",[],[],{"id":78,"title":81,"date":82,"time":83,"tags":99},[64,85,18],[],"mini_r4orin",{"id":101,"data":103,"body":109,"filePath":110,"digest":111,"rendered":112},{"id":101,"title":104,"date":105,"time":106,"tags":107},"The Framework Dilema","2026-01-13","11:19",[64,18,108],"experience","I'm working on a project that involves running a YOLO model on the web using **ONNX Runtime Web**. On paper, it sounded straightforward: load the model, run inference, detect people. In practice, it turned into one of the most educational (and humbling) experiences Iâ€™ve had so far.\n\nI started the way I usually do with personal projects, dive in headfirst. I planned just enough to get moving, implemented things quickly, fixed errors as they came up, and tried to squeeze out as much performance as possible. This approach has always worked for me. And honestly, thereâ€™s nothing wrong with it.\n\nBut this project was different.\n\nThis wasnâ€™t just about â€œmaking it fast.â€ It was about understanding **why it wasnâ€™t fast** and more importantly, how performance is actually measured and reasoned about in real-world software.\n\n---\n\n## When Numbers Donâ€™t Make Sense\n\nMy initial inference time was around **5000 ms**. Thatâ€™s bad. Detection lagged, the experience felt broken, and no amount of surface-level tweaking seemed to help. My instinct was to assume something was wrong with ONNX, the model, or even the browser.\n\nThatâ€™s when I was taught something crucial. You map everything out. Every step. Every call. Every dependency.\n\nThat sounds obvious but itâ€™s genuinely hard when youâ€™ve never worked on a client-facing project before. For the first time, I wasnâ€™t just optimizing for myself. I was dealing with **real constraints, real expectations, and real people**. That pressure changes how you think.\n\nOnce I slowed down and actually mapped out the pipeline, things began to click. And honestly, I was more annoyed at myself than anything else because the answer was right there.\n\n---\n\n## Vanilla JS vs Frameworks: The Real Difference\n\nTo isolate the issue, I compared my implementation with another project running the **same YOLOv12n model**, doing the same task (person detection). That project used **Next.js**. Their inference time? Around **150 ms**.\n\nMine? Still stuck around **1500 ms**. The difference wasnâ€™t the model.  \nIt wasnâ€™t ONNX. It wasnâ€™t even the browser.\n\nIt was **how things were loaded and orchestrated**.\n\nIn my vanilla JavaScript setup, libraries were pulled in via CDN in a largely sequential manner. Scripts loaded step-by-step, dependencies resolved one after another, and the runtime paid the price.\n\nFrameworks like Next.js, on the other hand, do a _lot_ of invisible work for you:\n\n- Asynchronous and parallel loading of packages    \n- Smarter bundling and dependency resolution    \n- Optimized preprocessing pipelines    \n- Runtime optimizations that you donâ€™t even think about until you miss them    \n\nOnce I understood this, the performance gap stopped feeling mysterious and started feeling inevitable.\n\n---\n\n## What This Taught Me About Frameworks\n\nWe throw around the word _framework_ all the time. â€œUse React.â€ â€œUse Next.â€ â€œFrameworks make things faster.â€\n\nBut **why**? This project finally answered that for me.\n\nFrameworks arenâ€™t magic. Theyâ€™re distilled experience. They encode years of lessons about loading strategies, execution order, caching, and runtime behavior things that are incredibly hard to get right on your own unless youâ€™ve already made every mistake once.\n\nOnce you understand _what_ a framework is actually doing for you, adapting to new tools becomes much easier. You stop treating them as black boxes and start seeing them as systems with advantages and trade-offs.\n\n---\n\n## The Bigger Takeaway\n\nThis experience taught me a lot about Development vs production thinking. About how you make something just to work vs making it reliable and more importantly it taught the consequences of optimizing blindy instead of with intent.   \n\nItâ€™s not the end of this project. But now I know itâ€™s not about randomly optimizing or blaming the model. Itâ€™s about understanding the full pipeline: how things load, when they execute, and where time is actually spent.\n\nOnce that clicks, the problem stops feeling chaotic. Performance becomes something I can reason about, test systematically, and improve with intent.\n\nTo better understand everything end-to-end, I used **NotebookLM**, and it turned out to be genuinely enlightening.\n\nIf youâ€™re curious, check it out here: ðŸ‘‰ [NotebookLM](https://notebooklm.google.com/notebook/ee800966-4605-4691-8492-79f27bc89f7c)","src/content/posts/mini_r4orin.md","de8a1a57f5683e7a",{"html":113,"metadata":114},"\u003Cp>Iâ€™m working on a project that involves running a YOLO model on the web using \u003Cstrong>ONNX Runtime Web\u003C/strong>. On paper, it sounded straightforward: load the model, run inference, detect people. In practice, it turned into one of the most educational (and humbling) experiences Iâ€™ve had so far.\u003C/p>\n\u003Cp>I started the way I usually do with personal projects, dive in headfirst. I planned just enough to get moving, implemented things quickly, fixed errors as they came up, and tried to squeeze out as much performance as possible. This approach has always worked for me. And honestly, thereâ€™s nothing wrong with it.\u003C/p>\n\u003Cp>But this project was different.\u003C/p>\n\u003Cp>This wasnâ€™t just about â€œmaking it fast.â€ It was about understanding \u003Cstrong>why it wasnâ€™t fast\u003C/strong> and more importantly, how performance is actually measured and reasoned about in real-world software.\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"when-numbers-dont-make-sense\">When Numbers Donâ€™t Make Sense\u003C/h2>\n\u003Cp>My initial inference time was around \u003Cstrong>5000 ms\u003C/strong>. Thatâ€™s bad. Detection lagged, the experience felt broken, and no amount of surface-level tweaking seemed to help. My instinct was to assume something was wrong with ONNX, the model, or even the browser.\u003C/p>\n\u003Cp>Thatâ€™s when I was taught something crucial. You map everything out. Every step. Every call. Every dependency.\u003C/p>\n\u003Cp>That sounds obvious but itâ€™s genuinely hard when youâ€™ve never worked on a client-facing project before. For the first time, I wasnâ€™t just optimizing for myself. I was dealing with \u003Cstrong>real constraints, real expectations, and real people\u003C/strong>. That pressure changes how you think.\u003C/p>\n\u003Cp>Once I slowed down and actually mapped out the pipeline, things began to click. And honestly, I was more annoyed at myself than anything else because the answer was right there.\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"vanilla-js-vs-frameworks-the-real-difference\">Vanilla JS vs Frameworks: The Real Difference\u003C/h2>\n\u003Cp>To isolate the issue, I compared my implementation with another project running the \u003Cstrong>same YOLOv12n model\u003C/strong>, doing the same task (person detection). That project used \u003Cstrong>Next.js\u003C/strong>. Their inference time? Around \u003Cstrong>150 ms\u003C/strong>.\u003C/p>\n\u003Cp>Mine? Still stuck around \u003Cstrong>1500 ms\u003C/strong>. The difference wasnâ€™t the model.\u003Cbr>\nIt wasnâ€™t ONNX. It wasnâ€™t even the browser.\u003C/p>\n\u003Cp>It was \u003Cstrong>how things were loaded and orchestrated\u003C/strong>.\u003C/p>\n\u003Cp>In my vanilla JavaScript setup, libraries were pulled in via CDN in a largely sequential manner. Scripts loaded step-by-step, dependencies resolved one after another, and the runtime paid the price.\u003C/p>\n\u003Cp>Frameworks like Next.js, on the other hand, do a \u003Cem>lot\u003C/em> of invisible work for you:\u003C/p>\n\u003Cul>\n\u003Cli>Asynchronous and parallel loading of packages\u003C/li>\n\u003Cli>Smarter bundling and dependency resolution\u003C/li>\n\u003Cli>Optimized preprocessing pipelines\u003C/li>\n\u003Cli>Runtime optimizations that you donâ€™t even think about until you miss them\u003C/li>\n\u003C/ul>\n\u003Cp>Once I understood this, the performance gap stopped feeling mysterious and started feeling inevitable.\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"what-this-taught-me-about-frameworks\">What This Taught Me About Frameworks\u003C/h2>\n\u003Cp>We throw around the word \u003Cem>framework\u003C/em> all the time. â€œUse React.â€ â€œUse Next.â€ â€œFrameworks make things faster.â€\u003C/p>\n\u003Cp>But \u003Cstrong>why\u003C/strong>? This project finally answered that for me.\u003C/p>\n\u003Cp>Frameworks arenâ€™t magic. Theyâ€™re distilled experience. They encode years of lessons about loading strategies, execution order, caching, and runtime behavior things that are incredibly hard to get right on your own unless youâ€™ve already made every mistake once.\u003C/p>\n\u003Cp>Once you understand \u003Cem>what\u003C/em> a framework is actually doing for you, adapting to new tools becomes much easier. You stop treating them as black boxes and start seeing them as systems with advantages and trade-offs.\u003C/p>\n\u003Chr>\n\u003Ch2 id=\"the-bigger-takeaway\">The Bigger Takeaway\u003C/h2>\n\u003Cp>This experience taught me a lot about Development vs production thinking. About how you make something just to work vs making it reliable and more importantly it taught the consequences of optimizing blindy instead of with intent.\u003C/p>\n\u003Cp>Itâ€™s not the end of this project. But now I know itâ€™s not about randomly optimizing or blaming the model. Itâ€™s about understanding the full pipeline: how things load, when they execute, and where time is actually spent.\u003C/p>\n\u003Cp>Once that clicks, the problem stops feeling chaotic. Performance becomes something I can reason about, test systematically, and improve with intent.\u003C/p>\n\u003Cp>To better understand everything end-to-end, I used \u003Cstrong>NotebookLM\u003C/strong>, and it turned out to be genuinely enlightening.\u003C/p>\n\u003Cp>If youâ€™re curious, check it out here: ðŸ‘‰ \u003Ca href=\"https://notebooklm.google.com/notebook/ee800966-4605-4691-8492-79f27bc89f7c\">NotebookLM\u003C/a>\u003C/p>",{"headings":115,"localImagePaths":129,"remoteImagePaths":130,"frontmatter":131,"imagePaths":133},[116,120,123,126],{"depth":117,"slug":118,"text":119},2,"when-numbers-dont-make-sense","When Numbers Donâ€™t Make Sense",{"depth":117,"slug":121,"text":122},"vanilla-js-vs-frameworks-the-real-difference","Vanilla JS vs Frameworks: The Real Difference",{"depth":117,"slug":124,"text":125},"what-this-taught-me-about-frameworks","What This Taught Me About Frameworks",{"depth":117,"slug":127,"text":128},"the-bigger-takeaway","The Bigger Takeaway",[],[],{"id":101,"title":104,"date":105,"time":106,"tags":132},[64,18,108],[],"mini_t043s4",{"id":134,"data":136,"body":141,"filePath":142,"digest":143,"rendered":144},{"id":134,"title":137,"date":138,"time":139,"tags":140},"A Stand Still : Lumex Project","2025-09-02","21:44",[18,19,20],"Today I was working on my big project, a multimodal AI application. For this, Iâ€™m using a Luxonis OAK-D Pro, which combines a depth sensor and a camera. The idea is simple: it provides more data than a regular camera, which is why I bought one in the first place.\n\nBut the title *â€œstand stillâ€* sums up exactly where I am right now.\n\nLast week, I thought I was routing the video stream directly to my phone. In reality, it was going from the camera â†’ PC â†’ phone. My first goal was a proper round trip, but Iâ€™ve hit a literal stand still right at the beginning.\n\nAfter digging deeper, I found out that I couldnâ€™t just use a web server or web app to connect. The OAK-D needs a **physical connection** between the app and the camera. I looked into some GitHub repos that supposedly did this, but theyâ€™re all 4+ years old and broken. I even tried Chaquopy and a bunch of random approaches, hoping something would stick.\n\nFinally, I stripped it all down and aimed for the bare minimum: just get the RGB stream working. After about three hours, I managed to build an app that _ran without crashing_. But it still couldnâ€™t connect to the camera, and thatâ€™s where my progress stands.\n\nTalking it over with others made me realize something important: Iâ€™ve been shooting in the dark and just hoping things would work. Thatâ€™s not uncommon, especially with how much AI and LLMs can â€œfill in the blanksâ€ for you. But at some point, the shortcuts collapse.\n\nNow I see the real path forward: I need to break this down even further. I have to understand the **protocols, pipelines, and data types** at play. Only then will I actually be able to make this work.\n\nAll of the the failing, the learning, the breaking down happened in a single day. But I know I wonâ€™t feel fine until I get it running. For clarity (and maybe some sanity), I will keep updating my progress on Minis.","src/content/posts/mini_t043s4.md","efd08a3cc83ba5cc",{"html":145,"metadata":146},"\u003Cp>Today I was working on my big project, a multimodal AI application. For this, Iâ€™m using a Luxonis OAK-D Pro, which combines a depth sensor and a camera. The idea is simple: it provides more data than a regular camera, which is why I bought one in the first place.\u003C/p>\n\u003Cp>But the title \u003Cem>â€œstand stillâ€\u003C/em> sums up exactly where I am right now.\u003C/p>\n\u003Cp>Last week, I thought I was routing the video stream directly to my phone. In reality, it was going from the camera â†’ PC â†’ phone. My first goal was a proper round trip, but Iâ€™ve hit a literal stand still right at the beginning.\u003C/p>\n\u003Cp>After digging deeper, I found out that I couldnâ€™t just use a web server or web app to connect. The OAK-D needs a \u003Cstrong>physical connection\u003C/strong> between the app and the camera. I looked into some GitHub repos that supposedly did this, but theyâ€™re all 4+ years old and broken. I even tried Chaquopy and a bunch of random approaches, hoping something would stick.\u003C/p>\n\u003Cp>Finally, I stripped it all down and aimed for the bare minimum: just get the RGB stream working. After about three hours, I managed to build an app that \u003Cem>ran without crashing\u003C/em>. But it still couldnâ€™t connect to the camera, and thatâ€™s where my progress stands.\u003C/p>\n\u003Cp>Talking it over with others made me realize something important: Iâ€™ve been shooting in the dark and just hoping things would work. Thatâ€™s not uncommon, especially with how much AI and LLMs can â€œfill in the blanksâ€ for you. But at some point, the shortcuts collapse.\u003C/p>\n\u003Cp>Now I see the real path forward: I need to break this down even further. I have to understand the \u003Cstrong>protocols, pipelines, and data types\u003C/strong> at play. Only then will I actually be able to make this work.\u003C/p>\n\u003Cp>All of the the failing, the learning, the breaking down happened in a single day. But I know I wonâ€™t feel fine until I get it running. For clarity (and maybe some sanity), I will keep updating my progress on Minis.\u003C/p>",{"headings":147,"localImagePaths":148,"remoteImagePaths":149,"frontmatter":150,"imagePaths":152},[],[],[],{"id":134,"title":137,"date":138,"time":139,"tags":151},[18,19,20],[],"mini_tsgt9b",{"id":153,"data":155,"body":162,"filePath":163,"digest":164,"rendered":165},{"id":153,"title":156,"date":157,"time":158,"tags":159},"Triple Threat","2025-08-21","21:59",[160,161],"movie","movie review","I ended up watching *Venom 3: The Last Dance*, *Kraven the Hunter*, and *Madame Web* back to back. Venom was fine, but the other two, oh my god. Iâ€™d heard the talk about how bad they were when they first released, but seeing them myself was something else entirely. Some of them donâ€™t even warrant full reviews, so here are three short ones instead.\n\n---\n\n**Venom: The Last Dance (2024)**  \nItâ€™s dumb, but itâ€™s sweet and somehow thatâ€™s exactly why it works. Probably the only Sony Spider-Man villain to actually make it through a full trilogy, Venom goes out on a surprisingly fun and heartfelt note. The plot is decent, a little messy in places, but never boring. The dancing scene with Mrs. Chen? Adorable. And Eddie saying goodbye to Venom with *Memories* by Maroon 5 playing in the background unintentionally hilarious, but still kind of touching. The Venom trilogy might and will be Sonyâ€™s best work: fun, ridiculous, but never dull. **10/10**\n\n---\n\n**Kraven the Hunter (2024)**  \nNot even â€œfun bad,â€ just bad. Kraven feels like a Russian Tarzan knockoff, and not a good one. The twists are dumb, the characters are weak, and the whole thing drags on until you stop caring. Even cool-sounding characters like Chameleon, Foreigner, and Rhino are wasted and reduced to jokes. Completely forgettable. **1/10**\n\n---\n\n**Madame Web (2024)**  \nI made the mistake of watching this right after *Kraven the Hunter* and honestly, it made *Kraven* look like a masterpiece in comparison. This isnâ€™t just boring, itâ€™s boring *and* dumb. The plot is a mess, the dialogue is weak and action was so bad that a kid smashing action figures together wouldâ€™ve looked better. You canâ€™t help but wonder if this thing was made as a tax write-off. Worst of all, the cast deserved better they were probably told this was going to be the next big Marvel event, only for it to be the next big joke. **1/10**\n\n---\n\nWell, that was that. Six hours in total, and only two of them were actually fun. Skip *Kraven* and *Madame Web*, just watch *Venom 3*","src/content/posts/mini_tsgt9b.md","d50af85f964ab302",{"html":166,"metadata":167},"\u003Cp>I ended up watching \u003Cem>Venom 3: The Last Dance\u003C/em>, \u003Cem>Kraven the Hunter\u003C/em>, and \u003Cem>Madame Web\u003C/em> back to back. Venom was fine, but the other two, oh my god. Iâ€™d heard the talk about how bad they were when they first released, but seeing them myself was something else entirely. Some of them donâ€™t even warrant full reviews, so here are three short ones instead.\u003C/p>\n\u003Chr>\n\u003Cp>\u003Cstrong>Venom: The Last Dance (2024)\u003C/strong>\u003Cbr>\nItâ€™s dumb, but itâ€™s sweet and somehow thatâ€™s exactly why it works. Probably the only Sony Spider-Man villain to actually make it through a full trilogy, Venom goes out on a surprisingly fun and heartfelt note. The plot is decent, a little messy in places, but never boring. The dancing scene with Mrs. Chen? Adorable. And Eddie saying goodbye to Venom with \u003Cem>Memories\u003C/em> by Maroon 5 playing in the background unintentionally hilarious, but still kind of touching. The Venom trilogy might and will be Sonyâ€™s best work: fun, ridiculous, but never dull. \u003Cstrong>10/10\u003C/strong>\u003C/p>\n\u003Chr>\n\u003Cp>\u003Cstrong>Kraven the Hunter (2024)\u003C/strong>\u003Cbr>\nNot even â€œfun bad,â€ just bad. Kraven feels like a Russian Tarzan knockoff, and not a good one. The twists are dumb, the characters are weak, and the whole thing drags on until you stop caring. Even cool-sounding characters like Chameleon, Foreigner, and Rhino are wasted and reduced to jokes. Completely forgettable. \u003Cstrong>1/10\u003C/strong>\u003C/p>\n\u003Chr>\n\u003Cp>\u003Cstrong>Madame Web (2024)\u003C/strong>\u003Cbr>\nI made the mistake of watching this right after \u003Cem>Kraven the Hunter\u003C/em> and honestly, it made \u003Cem>Kraven\u003C/em> look like a masterpiece in comparison. This isnâ€™t just boring, itâ€™s boring \u003Cem>and\u003C/em> dumb. The plot is a mess, the dialogue is weak and action was so bad that a kid smashing action figures together wouldâ€™ve looked better. You canâ€™t help but wonder if this thing was made as a tax write-off. Worst of all, the cast deserved better they were probably told this was going to be the next big Marvel event, only for it to be the next big joke. \u003Cstrong>1/10\u003C/strong>\u003C/p>\n\u003Chr>\n\u003Cp>Well, that was that. Six hours in total, and only two of them were actually fun. Skip \u003Cem>Kraven\u003C/em> and \u003Cem>Madame Web\u003C/em>, just watch \u003Cem>Venom 3\u003C/em>\u003C/p>",{"headings":168,"localImagePaths":169,"remoteImagePaths":170,"frontmatter":171,"imagePaths":173},[],[],[],{"id":153,"title":156,"date":157,"time":158,"tags":172},[160,161],[],"mini_yfvw15",{"id":174,"data":176,"body":183,"filePath":184,"digest":185,"rendered":186},{"id":174,"title":177,"date":178,"time":179,"tags":180},"January 2026 Review â€” Getting Started","2026-02-03","12:05",[64,181,182],"life","projects","Welcome to a new series.  \nThis is a monthly review, sometimes short, sometimes long, always messy. The goal is simple: reflect on what was done and figure out what needs to be done next. No rules. Letâ€™s get into it.\n\nJanuary flew by. Itâ€™s wild how so much happened, yet it still feels like the year started yesterday.\n\nOn the blog front, I only published two posts: my [2025 yearly recap](https://medium.com/@beyondmebtw/hello-2026-310901fa0fab) and [The *Running Man* movie review](https://medium.com/@beyondmebtw/the-running-man-movie-review-55bbc72897ab). I wanted to write more, but Iâ€™m still figuring out workâ€“life balance. Working from home, shifting responsibilities, and changing workflows make it harder than expected. That said, my internship is genuinely rewarding I worked on two projects that made it to production. I learned a lot about production environments, backend APIs, databases, and UX. Even with some design background, itâ€™s easy to get stuck thinking like just a developer. Iâ€™m learning to catch those mistakes early and move forward.\n\nAs a part of work, Iâ€™ve been deep in computer vision research especially with the depth camera I have. Iâ€™ve been exploring world models, robotics, 3D object modeling, point clouds, inference engines, and compression patterns. Itâ€™s niche, itâ€™s complex, and itâ€™s exactly what I want to be doing. Thereâ€™s more to explore, and Iâ€™m not stopping.\n\nProcrastination, though? Heavy.  \nIâ€™ve wanted to make changes across my platform, expand things, and push forward but inertia crept in. This is creative work, and sometimes even the things you love start to feel like chores. I donâ€™t hate what I do, but that mental block exists. The way out has been routine, forcing structure where motivation dips. Thatâ€™s partly why this monthly review exists. No more waiting. Letâ€™s just get it done.\n\nFebruary is stacked.  \nBackend and frontend revamps for the Minis platform. A content server overhaul. Simplifying my main site (uniform styling is already underway). Writing pending reviewsâ€” The 2025 F1 season, *Parasite*, *Now You See Me 3*. F1 Preseason testing is around the corner, *Drive to Survive* Season 8 is coming, and thatâ€™ll be very interesting.\n\nTo conclude,\nLetâ€™s get started, baby.","src/content/posts/mini_yfvw15.md","fa01bf420e9ff943",{"html":187,"metadata":188},"\u003Cp>Welcome to a new series.\u003Cbr>\nThis is a monthly review, sometimes short, sometimes long, always messy. The goal is simple: reflect on what was done and figure out what needs to be done next. No rules. Letâ€™s get into it.\u003C/p>\n\u003Cp>January flew by. Itâ€™s wild how so much happened, yet it still feels like the year started yesterday.\u003C/p>\n\u003Cp>On the blog front, I only published two posts: my \u003Ca href=\"https://medium.com/@beyondmebtw/hello-2026-310901fa0fab\">2025 yearly recap\u003C/a> and \u003Ca href=\"https://medium.com/@beyondmebtw/the-running-man-movie-review-55bbc72897ab\">The \u003Cem>Running Man\u003C/em> movie review\u003C/a>. I wanted to write more, but Iâ€™m still figuring out workâ€“life balance. Working from home, shifting responsibilities, and changing workflows make it harder than expected. That said, my internship is genuinely rewarding I worked on two projects that made it to production. I learned a lot about production environments, backend APIs, databases, and UX. Even with some design background, itâ€™s easy to get stuck thinking like just a developer. Iâ€™m learning to catch those mistakes early and move forward.\u003C/p>\n\u003Cp>As a part of work, Iâ€™ve been deep in computer vision research especially with the depth camera I have. Iâ€™ve been exploring world models, robotics, 3D object modeling, point clouds, inference engines, and compression patterns. Itâ€™s niche, itâ€™s complex, and itâ€™s exactly what I want to be doing. Thereâ€™s more to explore, and Iâ€™m not stopping.\u003C/p>\n\u003Cp>Procrastination, though? Heavy.\u003Cbr>\nIâ€™ve wanted to make changes across my platform, expand things, and push forward but inertia crept in. This is creative work, and sometimes even the things you love start to feel like chores. I donâ€™t hate what I do, but that mental block exists. The way out has been routine, forcing structure where motivation dips. Thatâ€™s partly why this monthly review exists. No more waiting. Letâ€™s just get it done.\u003C/p>\n\u003Cp>February is stacked.\u003Cbr>\nBackend and frontend revamps for the Minis platform. A content server overhaul. Simplifying my main site (uniform styling is already underway). Writing pending reviewsâ€” The 2025 F1 season, \u003Cem>Parasite\u003C/em>, \u003Cem>Now You See Me 3\u003C/em>. F1 Preseason testing is around the corner, \u003Cem>Drive to Survive\u003C/em> Season 8 is coming, and thatâ€™ll be very interesting.\u003C/p>\n\u003Cp>To conclude,\nLetâ€™s get started, baby.\u003C/p>",{"headings":189,"localImagePaths":190,"remoteImagePaths":191,"frontmatter":192,"imagePaths":194},[],[],[],{"id":174,"title":177,"date":178,"time":179,"tags":193},[64,181,182],[]]